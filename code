import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
%matplotlib inline
from numpy import array
import seaborn as sns
import xgboost as xgb
from xgboost import plot_importance, plot_tree
from sklearn.metrics import mean_squared_error, mean_absolute_error
plt.style.use('fivethirtyeight')
df = pd.read_csv('/content/drive/My Drive/datalandslide.csv')
#removing NaN Values
df = df.drop(df.index[[0,1]])
df.head()
df.set_index('dates', inplace = True)
df.index = pd.to_datetime(df.index)
df.head()
#Fill the missing values

df = df.fillna(method = 'bfill')
# Normalization of dataset
mx = df.max()
mn = df.min()
dataset = (df-mn)/(mx-mn)
dataset.corr()
dataset = np.array(df)
def split_sequences(sequences, n_steps):
    X, y = list(), list()
    for i in range(len(sequences)):
# find the end of this pattern
        end_ix = i + n_steps
# check if we are beyond the dataset
        if end_ix > len(sequences):
            break
# gather input and output parts of the pattern
        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]
        X.append(seq_x)
        y.append(seq_y)
    return array(X), array(y)
n_steps = 3
# convert into input/output
X, y = split_sequences(dataset, n_steps)
print(X.shape, y.shape)
x_train = X[:700]
y_train = y[:700]
x_test = X[700:]
y_test = y[700:]
import os

import matplotlib.pyplot as plt
import numpy as np

from pandas import read_csv
from pandas import concat
from pandas import DataFrame
from keras.layers import Input
from numpy import array
from keras.models import Model
from keras.layers import Dense
from keras.layers.merge import concatenate
from keras.models import Sequential
from keras.layers import Dropout
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Flatten, Dropout
from keras.layers import LSTM
from keras.utils.vis_utils import plot_model
#from keras.utils import plot_model
from sklearn.metrics import mean_squared_error
from keras.models import load_model
from scipy import stats
import csv as csv
import copy
n_features = x_train.shape[2]
model_lstm = Sequential()

model_lstm.add(LSTM(32, activation = 'tanh',return_sequences = False, input_shape = (n_steps,n_features)))
#model.add(LSTM(16, activation = 'tanh',return_sequences = False))
model_lstm.add(Dropout(0.3))
model_lstm.add(Dense(32,activation = 'tanh'))
model_lstm.add(Dense(32,activation = 'tanh'))
model_lstm.add(Dense(32,activation = 'tanh'))
model_lstm.add(Dense(32,activation = 'tanh'))
model_lstm.add(Dense(32,activation = 'tanh'))
model_lstm.add(Dense(32,activation = 'tanh'))
model_lstm.add(Dropout(0.3))
model_lstm.add(Dense(1,activation = 'tanh'))
model_lstm.compile(loss = 'mse', optimizer = 'adam', metrics=['accuracy'])
history = model_lstm.fit(x_train, y_train, epochs = 128, validation_data=(x_test, y_test), batch_size = 5, verbose = 1, shuffle = False)
y_pred = model_lstm.predict(x_test)

n_features = x_train.shape[2]
model_cnn = Sequential()
model_cnn.add(Conv1D(filters=128, kernel_size=3, activation='tanh',padding = 'same', input_shape=(n_steps,n_features)))
model_cnn.add(Dropout(0.3))
model_cnn.add(MaxPooling1D(pool_size=2,padding = 'same'))
model_cnn.add(Flatten())
model_cnn.add(Dense(16, activation = 'tanh'))
model_cnn.add(Dense(1,activation = 'tanh'))
model_cnn.compile(optimizer='adam', loss= 'mse', metrics=['accuracy'])
history = model_cnn.fit(x_train, y_train, epochs = 64, validation_data=(x_test, y_test), batch_size = 10, verbose = 0, shuffle = False)
y_pred = model_cnn.predict(x_test)

n_features = 1
n_length = 22


x_train = x_train.reshape((x_train.shape[0],n_steps,n_length,n_features))
x_test = x_test.reshape((x_test.shape[0],n_steps,n_length,n_features))
model_cnnlstm = Sequential()
model_cnnlstm.add(TimeDistributed(Conv1D(filters=32, kernel_size=5, padding ='same',activation='tanh'),input_shape=(None,n_length,n_features)))
model_cnnlstm.add(TimeDistributed(Flatten()))
model_cnnlstm.add(LSTM(64,activation = 'tanh'))
model_cnnlstm.add(Dropout(0.3))
model_cnnlstm.add(Dense(1,activation = 'tanh'))
model_cnnlstm.compile(optimizer = 'adam', loss = 'mse', metrics=['accuracy'])
history = model_cnnlstm.fit(x_train, y_train, epochs = 64, validation_data=(x_test, y_test), batch_size = 15, verbose = 0, shuffle = False)

n_features = 1
n_length = 22
x_train = x_train.reshape((x_train.shape[0],n_steps,1,n_length,n_features))
x_test = x_test.reshape((x_test.shape[0],n_steps,1,n_length,n_features))
model_convlstm = Sequential()
model_convlstm.add(ConvLSTM2D(filters=128, kernel_size=(1,5),input_shape=(n_steps, 1, n_length, n_features)))
model_convlstm.add(Dropout(0.3))
model_convlstm.add(Flatten())
model_convlstm.add(Dense(64, activation='tanh'))
model_convlstm.add(Dropout(0.3))
model_convlstm.add(Dense(1, activation='tanh'))
model_convlstm.compile(loss='mse', optimizer='adam', metrics=['accuracy'])
history = model_convlstm.fit(x_train, y_train, epochs = 128, validation_data=(x_test, y_test), batch_size = 20, verbose = 0, shuffle = False)
